{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion: eval_model\n",
    "\n",
    "Esta función debe recibir un target, unas predicciones para ese target, un argumento que determine si el problema es de regresión o clasificación y una lista de métricas:\n",
    "* Si el argumento dice que el problema es de regresión, la lista de métricas debe admitir las siguientes etiquetas RMSE, MAE, MAPE, GRAPH.\n",
    "* Si el argumento dice que el problema es de clasificación, la lista de métrica debe admitir, ACCURACY, PRECISION, RECALL, CLASS_REPORT, MATRIX, MATRIX_RECALL, MATRIX_PRED, PRECISION_X, RECALL_X. En el caso de las _X, X debe ser una etiqueta de alguna de las clases admitidas en el target.\n",
    "\n",
    "Funcionamiento:\n",
    "* Para cada etiqueta en la lista de métricas:\n",
    "- RMSE, debe printar por pantalla y devolver el RMSE de la predicción contra el target.\n",
    "- MAE, debe pintar por pantalla y devolver el MAE de la predicción contra el target. \n",
    "- MAPE, debe pintar por pantalla y devolver el MAPE de la predcción contra el target. Si el MAPE no se pudiera calcular la función debe avisar lanzando un error con un mensaje aclaratorio\n",
    "- GRAPH, la función debe pintar una gráfica comparativa (scatter plot) del target con la predicción\n",
    "- ACCURACY, pintará el accuracy del modelo contra target y lo retornará.\n",
    "- PRECISION, pintará la precision media contra target y la retornará.\n",
    "- RECALL, pintará la recall media contra target y la retornará.\n",
    "- CLASS_REPORT, mostrará el classification report por pantalla.\n",
    "- MATRIX, mostrará la matriz de confusión con los valores absolutos por casilla.\n",
    "- MATRIX_RECALL, mostrará la matriz de confusión con los valores normalizados según el recall de cada fila (si usas ConfussionMatrixDisplay esto se consigue con normalize = \"true\")\n",
    "- MATRIX_PRED, mostrará la matriz de confusión con los valores normalizados según las predicciones por columna (si usas ConfussionMatrixDisplay esto se consigue con normalize = \"pred\")\n",
    "- PRECISION_X, mostrará la precisión para la clase etiquetada con el valor que sustituya a X (ej. PRECISION_0, mostrará la precisión de la clase 0)\n",
    "- RECALL_X, mostrará el recall para la clase etiquetada co nel valor que sustituya a X (ej. RECALL_red, mostrará el recall de la clase etiquetada como \"red\")\n",
    "\n",
    "NOTA1: Como puede que la función devuelva varias métricas, debe hacerlo en una tupla en el orden de aparición de la métrica en la lista que se le pasa como argumento. Ejemplo si la lista de entrada es [\"GRAPH\",\"RMSE\",\"MAE\"], la fución pintará la comparativa, imprimirá el RMSE y el MAE (da igual que lo haga antes de dibujar la gráfica) y devolverá una tupla con el (RMSE,MAE) por ese orden.\n",
    "NOTA2: Una lista para clasificación puede contener varias PRECISION_X y RECALL_X, pej [\"PRECISION_red\",\"PRECISION_white\",\"RECALL_red\"] es una lista válida, tendrá que devolver la precisión de \"red\", la de \"white\" y el recall de \"red\". Si algunas de las etiquetas no existe debe arrojar ese error y detener el funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def eval_model(target, predictions, problem_type, metrics):\n",
    "\n",
    "    \"\"\"\n",
    "    Evalúa modelos de preducción según las métricas especificadas en función de si son problemas de regresión o clasificación.\n",
    "\n",
    "    Argumentos:\n",
    "    - df (DataFrame): DataFrame de entrada.\n",
    "    - target (array): Columna objetivo del test (y_test).\n",
    "    - predictions (array): Predicciones de la columna objetivo resultado de la ejecución del modelo.\n",
    "    - problem_type (str): Indica si se trata de un problema de regresión o clasificación.\n",
    "    - metrics (list): Lista de métricas según la tipología de problema.\n",
    "\n",
    "    Retorna:\n",
    "    - una tupla con los resultados de las métricas deseadas (metrics) por orden.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "# Creamos una lista vacía donde se irán guardando los resultados de las métricas por su orden en metrics.\n",
    "    results = []\n",
    "\n",
    "# Creamos un primer condicional según la tipología de problema y para cada metrica de metrics haremos la función deseada además de registrar el valor correspondiente en la lusta results.   \n",
    "# Alternativa de tipología de regresión.\n",
    "    if problem_type == \"regression\":\n",
    "        print(\"Problema de regresión\")\n",
    "# Recorremos la lista de metrics para analizarla y guardar sus resultados por orden en la lista results.\n",
    "        for metric in metrics:\n",
    "# Si la métrica es RMSE, se calcula, se muestra por pantalla y se añade a la lista results.\n",
    "            if metric == \"RMSE\":\n",
    "                rmse = mean_squared_error(target, predictions, squared=False)\n",
    "                print(f\"RMSE: {rmse}\")\n",
    "                results.append(rmse)\n",
    "# Si la métrica es MAE, se calcula, se muestra por pantalla y se añade a la lista results.            \n",
    "            if metric == \"MAE\":\n",
    "                mae = mean_absolute_error(target, predictions)\n",
    "                print(f\"MAE: {mae}\")\n",
    "                results.append(mae)\n",
    "# Si la métrica es MAPE, se calcula, se muestra por pantalla y se añade a la lista results. Si no se puede calcular, se imprime un mensaje indicándolo.           \n",
    "            if metric == \"MAPE\":\n",
    "                try:\n",
    "                    mape = mean_absolute_percentage_error(target, predictions)\n",
    "                    print(f\"MAPE: {mape}\")\n",
    "                    results.append(mape)\n",
    "                except Exception as e:\n",
    "                    print(\"Error al calcular MAPE:\", e)\n",
    "                    results.append(None)\n",
    "# Si la métrica es GRAPH, se muestra por pantalla el scatter plot.            \n",
    "            if metric == \"GRAPH\":\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.scatter(target, predictions, alpha=0.5)\n",
    "                plt.xlabel(\"Target\")\n",
    "                plt.ylabel(\"Predictions\")\n",
    "                plt.title(\"Scatter Plot of Target vs Predictions\")\n",
    "                plt.show()\n",
    "# Alternativa de tipología de clasificación.    \n",
    "    elif problem_type == \"classification\":\n",
    "        print(\"Problema de clasificación\")\n",
    "        for metric in metrics:\n",
    "# Si la métrica es Accuaracy, se calcula, se muestra por pantalla y se añade a la lista results.\n",
    "            if metric == \"ACCURACY\":\n",
    "                accuracy = accuracy_score(target, predictions)\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                results.append(accuracy)\n",
    "# Si la métrica es Precision, se calcula, se muestra por pantalla y se añade a la lista results.            \n",
    "            if metric == \"PRECISION\":\n",
    "                precision = precision_score(target, predictions, average='macro')\n",
    "                print(f\"Precision: {precision}\")\n",
    "                results.append(precision)\n",
    "# Si la métrica es Recall, se calcula, se muestra por pantalla y se añade a la lista results.            \n",
    "            if metric == \"RECALL\":\n",
    "                recall = recall_score(target, predictions, average='macro')\n",
    "                print(f\"Recall: {recall}\")\n",
    "                results.append(recall)\n",
    "# Si la métrica es Classification report, se muestra por pantalla.            \n",
    "            if metric == \"CLASS_REPORT\":\n",
    "                class_report = classification_report(target, predictions)\n",
    "                print(\"Classification Report:\")\n",
    "                print(class_report)\n",
    "# Si la métrica es Matrix, se muestra por pantalla la matriz de confusión con valores absolutos.            \n",
    "            if metric == \"MATRIX\":\n",
    "                matrix = confusion_matrix(target, predictions)\n",
    "                print(\"Confusion Matrix:\")\n",
    "                print(matrix)\n",
    "# Si la métrica es Matrix recall, se muestra por pantalla la matriz de confusión con valores relativos por fila.            \n",
    "            if metric == \"MATRIX_RECALL\":\n",
    "                matrix_recall = confusion_matrix(target, predictions, normalize='true')\n",
    "                print(\"Confusion Matrix (Normalized by Recall):\")\n",
    "                print(matrix_recall)\n",
    "# Si la métrica es Matrix prediction, se muestra por pantalla la matriz de confusión con valores relativos por columna.            \n",
    "            if metric == \"MATRIX_PRED\":\n",
    "                matrix_pred = confusion_matrix(target, predictions, normalize='pred')\n",
    "                print(\"Confusion Matrix (Normalized by Predictions):\")\n",
    "                print(matrix_pred)\n",
    "# Si la métrica demanda la precisión contra una variable concreta se calcula, se muestra por pantalla y se añade a la lista results. Si la variable no es ninguna de las columnas del dataframe se informa del error.\n",
    "# Pendietne de revisar!                        \n",
    "            if metric.startswith(\"PRECISION_\"):\n",
    "                label = metric.split(\"_\")[1]\n",
    "                precision_x = precision_score(target, predictions, labels=[label])\n",
    "                print(f\"Precision for {label}: {precision_x}\")\n",
    "                results.append(precision_x)\n",
    "# Si la métrica demanda el recall contra una variable concreta se calcula, se muestra por pantalla y se añade a la lista results. Si la variable no es ninguna de las columnas del dataframe se informa del error.            \n",
    "# Pendietne de revisar!               \n",
    "            if metric.startswith(\"RECALL_\"):\n",
    "                label = metric.split(\"_\")[1]\n",
    "                if label in df.columns():\n",
    "                    recall_x = recall_score(target, predictions, labels=[label])\n",
    "                    print(f\"Recall for {label}: {recall_x}\")\n",
    "                    results.append(recall_x)\n",
    "                else:\n",
    "                    print(f\"Error: {label} no pertenece al dataframe.\")\n",
    "    else:\n",
    "        print(\"Corrige la tipología de problema: regression o classification.\")\n",
    "\n",
    "# Retornamos una tupla con la lista de los resultados de cada métrica deseada por orden.    \n",
    "    return tuple(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   age  sibsp  parch     fare  class    who  adult_male  embark_town  \\\n",
       "0    male  22.0      1      0   7.2500  Third    man        True  Southampton   \n",
       "1  female  38.0      1      0  71.2833  First  woman       False    Cherbourg   \n",
       "2  female  26.0      0      0   7.9250  Third  woman       False  Southampton   \n",
       "3  female  35.0      1      0  53.1000  First  woman       False  Southampton   \n",
       "4    male  35.0      0      0   8.0500  Third    man        True  Southampton   \n",
       "\n",
       "  alive  alone  \n",
       "0    no  False  \n",
       "1   yes  False  \n",
       "2   yes   True  \n",
       "3   yes  False  \n",
       "4    no   True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sex',\n",
    "  'age',\n",
    "  'sibSp',\n",
    "  'fare',\n",
    "  'embark_town']\n",
    "target = \"alive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size= 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(target, axis = 1)\n",
    "X_test = test_set.drop(target, axis = 1)\n",
    "y_train = train_set[target]\n",
    "y_test = test_set[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scal = pd.get_dummies(X_train)\n",
    "X_test_scal = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.073611\n",
      "0:\tlearn: 0.6636243\ttotal: 1.92ms\tremaining: 190ms\n",
      "1:\tlearn: 0.6403321\ttotal: 4.09ms\tremaining: 200ms\n",
      "2:\tlearn: 0.6174038\ttotal: 5.28ms\tremaining: 171ms\n",
      "3:\tlearn: 0.5971421\ttotal: 6.53ms\tremaining: 157ms\n",
      "4:\tlearn: 0.5789508\ttotal: 8.03ms\tremaining: 153ms\n",
      "5:\tlearn: 0.5630033\ttotal: 9.23ms\tremaining: 145ms\n",
      "6:\tlearn: 0.5490139\ttotal: 10.3ms\tremaining: 136ms\n",
      "7:\tlearn: 0.5363719\ttotal: 12.9ms\tremaining: 149ms\n",
      "8:\tlearn: 0.5238149\ttotal: 14.4ms\tremaining: 145ms\n",
      "9:\tlearn: 0.5122729\ttotal: 15.7ms\tremaining: 141ms\n",
      "10:\tlearn: 0.5042010\ttotal: 16.3ms\tremaining: 132ms\n",
      "11:\tlearn: 0.4951858\ttotal: 17.7ms\tremaining: 130ms\n",
      "12:\tlearn: 0.4864746\ttotal: 18.9ms\tremaining: 126ms\n",
      "13:\tlearn: 0.4787811\ttotal: 20.2ms\tremaining: 124ms\n",
      "14:\tlearn: 0.4720348\ttotal: 20.6ms\tremaining: 117ms\n",
      "15:\tlearn: 0.4653409\ttotal: 21.8ms\tremaining: 115ms\n",
      "16:\tlearn: 0.4603952\ttotal: 24.2ms\tremaining: 118ms\n",
      "17:\tlearn: 0.4550179\ttotal: 26.5ms\tremaining: 121ms\n",
      "18:\tlearn: 0.4506999\ttotal: 27.8ms\tremaining: 118ms\n",
      "19:\tlearn: 0.4457760\ttotal: 29ms\tremaining: 116ms\n",
      "20:\tlearn: 0.4405068\ttotal: 30.1ms\tremaining: 113ms\n",
      "21:\tlearn: 0.4358152\ttotal: 31.3ms\tremaining: 111ms\n",
      "22:\tlearn: 0.4333584\ttotal: 32.4ms\tremaining: 108ms\n",
      "23:\tlearn: 0.4305227\ttotal: 33.6ms\tremaining: 106ms\n",
      "24:\tlearn: 0.4266057\ttotal: 34.6ms\tremaining: 104ms\n",
      "25:\tlearn: 0.4238517\ttotal: 35.8ms\tremaining: 102ms\n",
      "26:\tlearn: 0.4216597\ttotal: 36.8ms\tremaining: 99.5ms\n",
      "27:\tlearn: 0.4192015\ttotal: 37.9ms\tremaining: 97.5ms\n",
      "28:\tlearn: 0.4170006\ttotal: 39ms\tremaining: 95.5ms\n",
      "29:\tlearn: 0.4147829\ttotal: 40.1ms\tremaining: 93.5ms\n",
      "30:\tlearn: 0.4126222\ttotal: 41.1ms\tremaining: 91.6ms\n",
      "31:\tlearn: 0.4111912\ttotal: 42.2ms\tremaining: 89.7ms\n",
      "32:\tlearn: 0.4088992\ttotal: 43.2ms\tremaining: 87.7ms\n",
      "33:\tlearn: 0.4073613\ttotal: 44.4ms\tremaining: 86.2ms\n",
      "34:\tlearn: 0.4062951\ttotal: 45.1ms\tremaining: 83.7ms\n",
      "35:\tlearn: 0.4044793\ttotal: 46.8ms\tremaining: 83.3ms\n",
      "36:\tlearn: 0.4035710\ttotal: 48ms\tremaining: 81.8ms\n",
      "37:\tlearn: 0.4010999\ttotal: 49.1ms\tremaining: 80.2ms\n",
      "38:\tlearn: 0.3991049\ttotal: 50.2ms\tremaining: 78.6ms\n",
      "39:\tlearn: 0.3975290\ttotal: 51.3ms\tremaining: 77ms\n",
      "40:\tlearn: 0.3969599\ttotal: 51.8ms\tremaining: 74.5ms\n",
      "41:\tlearn: 0.3961600\ttotal: 52.8ms\tremaining: 72.9ms\n",
      "42:\tlearn: 0.3943271\ttotal: 53.8ms\tremaining: 71.3ms\n",
      "43:\tlearn: 0.3934327\ttotal: 54.9ms\tremaining: 69.8ms\n",
      "44:\tlearn: 0.3918954\ttotal: 55.9ms\tremaining: 68.3ms\n",
      "45:\tlearn: 0.3904833\ttotal: 57.1ms\tremaining: 67ms\n",
      "46:\tlearn: 0.3895612\ttotal: 58.2ms\tremaining: 65.6ms\n",
      "47:\tlearn: 0.3887102\ttotal: 59.2ms\tremaining: 64.1ms\n",
      "48:\tlearn: 0.3881137\ttotal: 60.2ms\tremaining: 62.7ms\n",
      "49:\tlearn: 0.3870898\ttotal: 61.3ms\tremaining: 61.3ms\n",
      "50:\tlearn: 0.3861149\ttotal: 62.5ms\tremaining: 60.1ms\n",
      "51:\tlearn: 0.3855042\ttotal: 63.4ms\tremaining: 58.5ms\n",
      "52:\tlearn: 0.3852121\ttotal: 63.9ms\tremaining: 56.6ms\n",
      "53:\tlearn: 0.3848628\ttotal: 64.7ms\tremaining: 55.1ms\n",
      "54:\tlearn: 0.3839563\ttotal: 65.2ms\tremaining: 53.4ms\n",
      "55:\tlearn: 0.3833722\ttotal: 66.1ms\tremaining: 51.9ms\n",
      "56:\tlearn: 0.3829587\ttotal: 67ms\tremaining: 50.5ms\n",
      "57:\tlearn: 0.3821013\ttotal: 67.8ms\tremaining: 49.1ms\n",
      "58:\tlearn: 0.3812078\ttotal: 68.7ms\tremaining: 47.7ms\n",
      "59:\tlearn: 0.3802091\ttotal: 69.5ms\tremaining: 46.3ms\n",
      "60:\tlearn: 0.3791730\ttotal: 70.3ms\tremaining: 45ms\n",
      "61:\tlearn: 0.3786084\ttotal: 71.2ms\tremaining: 43.6ms\n",
      "62:\tlearn: 0.3774868\ttotal: 72ms\tremaining: 42.3ms\n",
      "63:\tlearn: 0.3761347\ttotal: 73.1ms\tremaining: 41.1ms\n",
      "64:\tlearn: 0.3751960\ttotal: 74.1ms\tremaining: 39.9ms\n",
      "65:\tlearn: 0.3742592\ttotal: 75.1ms\tremaining: 38.7ms\n",
      "66:\tlearn: 0.3732378\ttotal: 76.2ms\tremaining: 37.5ms\n",
      "67:\tlearn: 0.3730860\ttotal: 76.7ms\tremaining: 36.1ms\n",
      "68:\tlearn: 0.3721948\ttotal: 77.7ms\tremaining: 34.9ms\n",
      "69:\tlearn: 0.3715446\ttotal: 78.8ms\tremaining: 33.8ms\n",
      "70:\tlearn: 0.3705695\ttotal: 79.9ms\tremaining: 32.6ms\n",
      "71:\tlearn: 0.3695902\ttotal: 81ms\tremaining: 31.5ms\n",
      "72:\tlearn: 0.3688118\ttotal: 82.1ms\tremaining: 30.4ms\n",
      "73:\tlearn: 0.3677028\ttotal: 83.3ms\tremaining: 29.3ms\n",
      "74:\tlearn: 0.3674603\ttotal: 84.4ms\tremaining: 28.1ms\n",
      "75:\tlearn: 0.3665261\ttotal: 86ms\tremaining: 27.2ms\n",
      "76:\tlearn: 0.3661041\ttotal: 87.1ms\tremaining: 26ms\n",
      "77:\tlearn: 0.3658529\ttotal: 88ms\tremaining: 24.8ms\n",
      "78:\tlearn: 0.3653541\ttotal: 88.9ms\tremaining: 23.6ms\n",
      "79:\tlearn: 0.3651187\ttotal: 89.9ms\tremaining: 22.5ms\n",
      "80:\tlearn: 0.3648034\ttotal: 90.9ms\tremaining: 21.3ms\n",
      "81:\tlearn: 0.3645962\ttotal: 92.1ms\tremaining: 20.2ms\n",
      "82:\tlearn: 0.3641360\ttotal: 93.7ms\tremaining: 19.2ms\n",
      "83:\tlearn: 0.3636270\ttotal: 95ms\tremaining: 18.1ms\n",
      "84:\tlearn: 0.3632926\ttotal: 95.9ms\tremaining: 16.9ms\n",
      "85:\tlearn: 0.3630583\ttotal: 97ms\tremaining: 15.8ms\n",
      "86:\tlearn: 0.3621542\ttotal: 97.9ms\tremaining: 14.6ms\n",
      "87:\tlearn: 0.3617142\ttotal: 98.8ms\tremaining: 13.5ms\n",
      "88:\tlearn: 0.3612990\ttotal: 99.7ms\tremaining: 12.3ms\n",
      "89:\tlearn: 0.3605682\ttotal: 101ms\tremaining: 11.2ms\n",
      "90:\tlearn: 0.3601620\ttotal: 102ms\tremaining: 10ms\n",
      "91:\tlearn: 0.3592773\ttotal: 102ms\tremaining: 8.9ms\n",
      "92:\tlearn: 0.3588183\ttotal: 103ms\tremaining: 7.77ms\n",
      "93:\tlearn: 0.3579490\ttotal: 104ms\tremaining: 6.65ms\n",
      "94:\tlearn: 0.3569859\ttotal: 105ms\tremaining: 5.53ms\n",
      "95:\tlearn: 0.3569115\ttotal: 106ms\tremaining: 4.42ms\n",
      "96:\tlearn: 0.3566123\ttotal: 107ms\tremaining: 3.31ms\n",
      "97:\tlearn: 0.3554563\ttotal: 108ms\tremaining: 2.21ms\n",
      "98:\tlearn: 0.3551953\ttotal: 109ms\tremaining: 1.1ms\n",
      "99:\tlearn: 0.3549612\ttotal: 111ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "modelo_cb = CatBoostClassifier(iterations=100, random_state=42)\n",
    "modelo_cb.fit(X_train_scal, y_train, cat_features=None)\n",
    "\n",
    "# Predecir sobre el conjunto de prueba\n",
    "predicciones_cb = modelo_cb.predict(X_test_scal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problema de clasificación\n",
      "Accuracy: 0.8100558659217877\n",
      "Confusion Matrix:\n",
      "[[92 13]\n",
      " [21 53]]\n",
      "Precision: 0.8085947975328507\n",
      "Recall: 0.7962033462033462\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.81      0.88      0.84       105\n",
      "         yes       0.80      0.72      0.76        74\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "Confusion Matrix (Normalized by Recall):\n",
      "[[0.87619048 0.12380952]\n",
      " [0.28378378 0.71621622]]\n",
      "Confusion Matrix (Normalized by Predictions):\n",
      "[[0.81415929 0.1969697 ]\n",
      " [0.18584071 0.8030303 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8100558659217877, 0.8085947975328507, 0.7962033462033462)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos funcionamiento de la función:\n",
    "eval_model(target=y_test,predictions=predicciones_cb,problem_type=\"classification\",metrics=[\"ACCURACY\",\"MATRIX\",\"PRECISION\",\"RECALL\",\"CLASS_REPORT\",\"MATRIX_RECALL\",\"MATRIX_PRED\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
